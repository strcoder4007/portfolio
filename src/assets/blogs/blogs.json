[
  {
    "title": "Adding real-time data to local LLM",
    "content": "<p>Large Language Models (LLMs) have undeniably transformed the landscape of artificial intelligence, offering unprecedented capabilities in natural language processing. However, these models face a significant limitation: their knowledge is confined to the data they were trained on, often leading to outdated information and an inability to engage with emerging topics. This article explores an innovative solution to this challenge, demonstrating how to integrate real-time web information into local LLMs, thereby substantially enhancing their performance and relevance.</p>\n<h2>The Inherent Limitation of Traditional LLMs</h2>\n<p>LLMs, despite their impressive abilities in text generation, language translation, and question-answering, are fundamentally restricted by their training data. This limitation manifests in several critical ways:</p>\n<ul>\n<li>Outdated Information: LLMs cannot access or incorporate new information that has emerged since their training.</li>\n<li>Limited Scope: They may lack knowledge in niche or rapidly evolving fields.</li>\n<li>Inconsistent Performance: Their effectiveness can vary significantly depending on the recency and relevance of the query to their training data.</li>\n</ul>\n<h2>Bridging the Knowledge Gap: The Searxng Solution</h2>\n<p>To address these limitations, we propose integrating Searxng, a privacy-focused metasearch engine, with local LLMs. This integration offers several significant advantages:</p>\n<ol>\n<li>Real-Time Information Access: LLMs can now query and incorporate the latest information from the web.</li>\n<li>Expanded Knowledge Base: The model's effective knowledge extends far beyond its original training data.</li>\n<li>Enhanced Accuracy: Responses are based on current, verified information from multiple sources.</li>\n<li>Improved Relevance: The LLM can now address queries on recent events and emerging topics with authority.</li>\n</ol>\n<h2>Technical Implementation: Integrating Searxng with Local LLMs</h2>\n<p>The integration process involves several key components:</p>\n<h3>1. Environment Setup</h3>\n<p>Begin by ensuring Ollama (an open-source LLM) and Open Web UI are operational. A docker-compose.yaml file is provided for efficient setup:</p>\n<pre style=\"background-color: #222; padding: 5px 10px\"><code>version: '3.8'\nservices:\n  ollama:\n    image: ollama/ollama:latest\n    ports:\n      - 11434:11434\n    volumes:\n      - /path/to/ollama/config:/root/.ollama\n  ollama-webui:\n    image: ghcr.io/ollama-webui/ollama-webui:main\n    volumes:\n      - /path/to/webui/config:/app/backend/data\n    ports:\n      - 8080:8080\n    environment:\n      - '/ollama/api=http://ollama:11434/api'\n</code></pre>\n<h3>2. Searxng Configuration</h3>\n<p>Configure Searxng by creating the necessary configuration files and adding a Searxng service to your Docker Compose file:</p>\n<pre style=\"background-color: #222; padding: 5px 10px\"><code>  searxng:\n    image: searxng/searxng:latest\n    ports:\n      - 8214:8080\n    volumes:\n      - ./searxng:/etc/searxng\n    environment:\n      - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/\n</code></pre>\n<h3>3. Integration with Open Web UI</h3>\n<p>In the Open Web UI Admin Panel:</p>\n<ol>\n<li>Navigate to the Web Search tab</li>\n<li>Enable Web Search</li>\n<li>Configure the Searxng Query URL (e.g., http://[YOUR_IP]:8214/search?q=<query>)</li>\n</ol>\n<h3>4. Activation in Chat Interface</h3>\n<p>For each new chat session in Open Web UI, enable the Web Search toggle to leverage Searxng's capabilities.</p>\n<h2>The Integration Process: Behind the Scenes</h2>\n<p>When a query is submitted, the following process occurs:</p>\n<ol>\n<li>The LLM analyzes the query to determine if web search is necessary.</li>\n<li>If required, Searxng performs a comprehensive search across multiple engines.</li>\n<li>Search results are returned to the LLM in a structured format.</li>\n<li>The LLM processes this information, extracting relevant data and insights.</li>\n<li>Finally, the LLM generates a response that incorporates both its base knowledge and the newly acquired information.</li>\n</ol>\n<h2>Implications and Future Directions</h2>\n<p>This integration represents a significant advancement in LLM technology, effectively creating a symbiosis between static trained models and dynamic web information. The implications of this development are far-reaching:</p>\n<ul>\n<li>Enhanced AI Assistants: More accurate and up-to-date responses in various applications.</li>\n<li>Improved Decision Support Systems: Access to real-time data for more informed decision-making.</li>\n<li>Adaptive Learning Systems: LLMs that can continually update their knowledge base.</li>\n</ul>\n<p>Future research directions may include optimizing the integration for speed and efficiency, developing more sophisticated information synthesis algorithms, and exploring ways to maintain privacy and data security in this connected system.</p>\n<h2>Conclusion</h2>\n<p>The integration of real-time web information with local LLMs marks a pivotal advancement in AI technology. By bridging the gap between static training data and dynamic web content, we unlock new potentials for these models, significantly enhancing their utility and relevance in our rapidly evolving digital landscape. As we continue to refine this integration, the possibilities for more intelligent, adaptive, and informative AI systems are boundless.</p>",
    "time": "02 July 2024",
    "tags": [
      "Visualizing layer",
      "Deep learning",
      "Convolutional Neural Networks"
    ]
  },
  {
    "title": "Instacode Online Judge",
    "content": "<h3>A Comprehensive Coding Platform for Educational Institutions</h3>\n\n<p>In 2016, as my 3rd year project, I presented this online judge to college. Soon, it was implemented as the new way to conduct practicals in Computer Science Department. Instacode online judge is a robust online coding platform developed for college practicals. Inspired by popular platforms like HackerRank, Instacode offers a unique environment for students to hone their programming skills across multiple languages while fostering healthy competition through a dynamic leaderboard system.</p>\n\n<h3>1. Project Overview</h3>\n\n<p>Instacode Online Judge is a PHP-based web application designed to:</p>\n<ul>\n    <li>Evaluate code submissions in 14 different programming languages</li>\n    <li>Provide immediate feedback on code correctness and efficiency</li>\n    <li>Rank participants based on their performance</li>\n    <li>Facilitate coding practice and assessment in educational settings</li>\n</ul>\n\n<h3>2. Key Features</h3>\n\n<h3>2.1 Multi-language Support</h3>\n<p>Instacode supports code evaluation in 14 programming languages, catering to a wide range of curricula and student preferences. This feature allows institutions to use the platform across various courses and skill levels. The supported languages include C, C++, Java, Python, JavaScript, Ruby, PHP, and more, ensuring that students can practice and be assessed in the languages most relevant to their coursework and future careers.</p>\n\n<h3>2.2 Automated Code Evaluation</h3>\n<p>The platform employs a robust evaluation engine that assesses submitted code for correctness, efficiency, and adherence to problem constraints. This automated process ensures fair and consistent grading across all submissions. The evaluation system runs the submitted code against a predefined set of test cases, checking for expected output, execution time, and resource usage. This comprehensive approach ensures that students receive detailed feedback on their code's performance.</p>\n\n<h3>2.3 Real-time Leaderboard</h3>\n<p>A dynamic leaderboard ranks participants based on their performance, introducing a gamification element that encourages healthy competition and motivates students to improve their skills. The leaderboard updates in real-time, reflecting the latest submissions and scores, and provides a transparent view of each student's progress relative to their peers. This feature not only motivates students but also adds an engaging and competitive aspect to coding practice.</p>\n\n<h3>2.4 User-friendly Interface</h3>\n<p>The interface is designed with simplicity and functionality in mind, allowing students and instructors to navigate the platform effortlessly. The clean and intuitive design ensures that users can easily access all features, from code submission and evaluation to leaderboard tracking and feedback review. The responsive layout ensures a seamless experience across various devices, including desktops, laptops, tablets, and smartphones.</p>\n\n<h3>3. Technical Implementation</h3>\n\n<h3>3.1 Backend Architecture</h3>\n<p>Instacode is built using PHP, a versatile server-side scripting language well-suited for web development. The choice of PHP allows for:</p>\n<ul>\n    <li>Efficient handling of concurrent user requests</li>\n    <li>Easy integration with various databases for storing user data, problem sets, and submissions</li>\n    <li>Scalability to accommodate growing user bases and increasing computational demands</li>\n</ul>\n<p>The backend architecture includes a RESTful API that facilitates communication between the frontend and the server. This modular approach ensures that different components of the system can interact seamlessly and be maintained or upgraded independently.</p>\n\n<h3>3.2 Code Execution and Evaluation</h3>\n<p>The core of Instacode's functionality lies in its code execution and evaluation system. This system involves:</p>\n<ul>\n    <li>Sandboxed environments for secure code execution</li>\n    <li>Time and memory constraints to evaluate code efficiency</li>\n    <li>Comparison of output with pre-defined test cases</li>\n    <li>Error handling and informative feedback generation</li>\n</ul>\n<p>Each submitted code snippet is executed in an isolated environment to prevent security breaches and ensure fair evaluation. The system uses Docker containers to create these sandboxes, providing a consistent and controlled environment for code execution. This isolation mechanism ensures that one student's code cannot interfere with the server or other submissions.</p>\n\n<h3>3.3 Database Design</h3>\n<p>An efficient database schema is crucial for managing:</p>\n<ul>\n    <li>User profiles and authentication</li>\n    <li>Problem sets and their associated test cases</li>\n    <li>Submission history and evaluation results</li>\n    <li>Leaderboard data</li>\n</ul>\n<p>The SQL database is designed to optimize performance and ensure data integrity. Key tables include <code>users</code>, <code>problems</code>, <code>submissions</code>, and <code>leaderboard</code>. Indexing and normalization techniques are employed to speed up queries and reduce redundancy, ensuring that the platform remains responsive even under heavy load.</p>\n\n<h3>4. Educational Impact</h3>\n\n<p>Instacode Online Judge offers several benefits in an educational context:</p>\n<ul>\n    <li><strong>Practical Skill Development:</strong> Students gain hands-on experience in writing, debugging, and optimizing code. This practical approach complements theoretical learning and helps students develop problem-solving skills that are crucial in the tech industry.</li>\n    <li><strong>Immediate Feedback:</strong> The platform's instant evaluation helps students learn from their mistakes quickly. Immediate feedback allows students to understand where their code went wrong and how to improve it, facilitating a faster and more effective learning process.</li>\n    <li><strong>Performance Metrics:</strong> Instructors can gauge student progress and identify areas needing additional focus. Detailed performance metrics and analytics provide insights into common challenges faced by students, enabling targeted interventions and support.</li>\n    <li><strong>Preparation for Technical Interviews:</strong> The platform mimics the environment often used in technical interviews, helping students prepare for future career opportunities. By practicing coding problems in a timed, competitive setting, students can build the confidence and skills needed to succeed in real-world technical interviews.</li>\n</ul>\n\n<h3>5. Challenges and Solutions</h3>\n\n<p>Developing a platform like Instacode presents several challenges:</p>\n\n<h3>5.1 Security</h3>\n<p><strong>Challenge:</strong> Executing user-submitted code poses significant security risks.<br>\n<strong>Solution:</strong> Implement strict sandboxing techniques, input validation, and resource limitations to prevent malicious code execution. The use of Docker containers for sandboxing ensures that code runs in a secure, isolated environment, while input validation and resource limits mitigate the risk of harmful code affecting the system.</p>\n\n<h3>5.2 Scalability</h3>\n<p><strong>Challenge:</strong> Managing concurrent code executions and evaluations can strain server resources.<br>\n<strong>Solution:</strong> Employ load balancing, caching mechanisms, and optimize database queries to handle increased user load efficiently. By distributing the load across multiple servers and using caching to reduce redundant computations, Instacode can scale to accommodate a growing number of users without compromising performance.</p>\n\n<h3>5.3 Language Support</h3>\n<p><strong>Challenge:</strong> Supporting and maintaining 14 different programming languages.<br>\n<strong>Solution:</strong> Modular design for language-specific components, allowing easy updates and additions of new languages. Each language's compiler or interpreter is encapsulated in a separate module, making it easy to update or add support for additional languages without affecting the overall system.</p>\n\n<h3>6. Future Enhancements</h3>\n\n<p>Potential areas for future development include:</p>\n<ul>\n    <li>Integration with learning management systems (LMS) for seamless incorporation into course structures. This integration would allow instructors to manage assignments, grades, and student progress from a single platform, streamlining the educational process.</li>\n    <li>Advanced analytics for tracking student progress over time. By analyzing performance data, educators can identify trends, strengths, and areas for improvement, tailoring their teaching strategies to better meet student needs.</li>\n    <li>Collaborative coding features for pair programming exercises. Enabling students to work together on coding problems can foster teamwork and communication skills, which are essential in professional software development.</li>\n    <li>AI-powered hint system to guide students through problem-solving processes. An intelligent hint system could provide personalized guidance based on a student's progress and common mistakes, helping them overcome challenges more effectively.</li>\n</ul>\n\n<h3>7. Conclusion</h3>\n\n<p>Instacode Online Judge represents a significant step forward in computer science education tools. By providing a platform that combines practical coding experience with competitive elements, it offers an engaging and effective learning environment. As coding skills continue to be in high demand across various industries, tools like Instacode play a crucial role in preparing students for successful careers in technology.</p>\n\n<p>The development of Instacode not only demonstrates technical proficiency in web development and system design but also shows a deep understanding of educational needs in the computer science field. As the platform evolves, it has the potential to become an integral part of coding education, bridging the gap between theoretical knowledge and practical application. By continuously improving and expanding its features, Instacode aims to provide a comprehensive and versatile tool for both students and educators, enhancing the overall learning experience in computer science education.</p>",
    "time": "14 November 2017",
    "tags": ["Online Judge", "College practicle", "3rd year project"]
  },
  {
    "title": "Visualizing Layers while training",
    "content": "<p>In deep learning, understanding the internal workings of neural networks is pivotal for model interpretation, debugging, and optimization. This blog post delves into visualizing hidden layers of convolutional neural networks (CNNs) using PyTorch and Matplotlib, offering insights into the feature extraction process and model behavior.</p>\n\n<h3>1. Introduction</h3>\n\n<p>Deep CNNs have achieved exceptional performance in various computer vision tasks. However, their intricate architectures often obscure their decision-making processes. Visualizing the activations of hidden layers can illuminate the features and patterns learned by these networks, providing a clearer view of their functionality.</p>\n\n<h3>2. Data Collection and Processing</h3>\n\n<p>For this project, we utilized the MNIST dataset, a widely used benchmark in the field of image classification. This dataset consists of 70,000 handwritten digit images (60,000 for training and 10,000 for testing). We chose MNIST due to its simplicity and the ease with which we can test our visualization techniques.</p>\n\n<p>The data was collected from the torchvision library, which provides an easy-to-use interface for loading and transforming datasets. We applied basic preprocessing, including normalization, to ensure the images were suitable for our CNN. Normalization helps in speeding up the training process and improving the model's performance.</p>\n\n<h3>3. Implementation Overview</h3>\n\n<p>The implementation comprises three main components:</p>\n<ol>\n    <li>Building and training a CNN with PyTorch</li>\n    <li>Extracting activations from hidden layers</li>\n    <li>Visualizing these activations using Matplotlib</li>\n</ol>\n\n<h3>4. Setting Up the Environment</h3>\n\n<p>We begin by importing the necessary libraries:</p>\n\n<pre style=\"background-color: #222; padding: 5px 10px\"><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np</code></pre>\n\n<h3>5. Constructing the CNN Architecture</h3>\n\n<p>We defined a simple CNN with two convolutional layers followed by two fully connected layers:</p>\n\n<pre style=\"background-color: #222; padding: 5px 10px\"><code>class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = torch.relu(torch.max_pool2d(self.conv1(x), 2))\n        x = torch.relu(torch.max_pool2d(self.conv2(x), 2))\n        x = x.view(-1, 320)\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\nnet = Net()</code></pre>\n\n<h3>6. Training the Model</h3>\n\n<p>We trained the model using the MNIST dataset with the following setup:</p>\n\n<pre style=\"background-color: #222; padding: 5px 10px\"><code># Load MNIST dataset\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\ntrain_loader = torch.utils.data.DataLoader(\n    torchvision.datasets.MNIST('./data', train=True, download=True, transform=transform),\n    batch_size=64, shuffle=True)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n\n# Training loop\nfor epoch in range(5):\n    running_loss = 0.0\n    for i, data in enumerate(train_loader, 0):\n        inputs, labels = data\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader):.3f}')</code></pre>\n\n<p>During training, we monitored the loss to gauge the model's performance. After 5 epochs, the model demonstrated a reasonable convergence, though further tuning and longer training might improve results.</p>\n\n<h3>7. Model Selection and Evaluation</h3>\n\n<p>We initially experimented with the following models:</p>\n<ul>\n    <li>**Basic CNN**: As outlined above, with two convolutional layers and two fully connected layers.</li>\n    <li>**Deeper CNN**: Added more convolutional layers and increased filter sizes.</li>\n    <li>**Pre-trained Models**: We tried using pre-trained models like VGG and ResNet for feature extraction.</li>\n</ul>\n\n<p>The basic CNN served as a good starting point. While the deeper CNN showed promise, it required more computational resources and tuning. Surprisingly, the pre-trained models offered valuable insights but were less effective in this context due to MNIST's simplicity.</p>\n\n<p>Evaluation was performed using the test set, measuring accuracy and loss. The basic CNN model showed good performance with an accuracy of around 98%, whereas the deeper CNN achieved marginally better accuracy but with increased training time.</p>\n\n<h3>8. Visualizing Hidden Layer Activations</h3>\n\n<p>We implemented a function to visualize activations from the first convolutional layer:</p>\n\n<pre style=\"background-color: #222; padding: 5px 10px\"><code>def visualize_layer(layer, input_tensor):\n    # Forward pass through the layer\n    activation = layer(input_tensor)\n    \n    # Convert to numpy array\n    act_imshow = activation.detach().cpu().numpy()\n    \n    # Create subplots for each channel\n    fig, axs = plt.subplots(2, 5, figsize=(12, 6))\n    fig.suptitle('Feature Maps of First Convolutional Layer')\n    for idx in range(10):\n        ax = axs[idx//5, idx%5]\n        ax.imshow(act_imshow[0, idx], cmap='viridis')\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n# Obtain a sample image\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\n\n# Visualize the first convolutional layer\nvisualize_layer(net.conv1, images[0:1])</code></pre>\n\n<h3>9. Interpreting the Visualizations</h3>\n\n<p>The visualizations show the feature maps from the first convolutional layer. Brighter areas indicate stronger activations, revealing where the network detects features. Typically, early layers capture simple features like edges and textures, while deeper layers identify complex patterns specific to the task.</p>\n\n<h3>10. Advanced Visualization Techniques</h3>\n\n<p>While basic visualizations are helpful, advanced techniques can offer deeper insights:</p>\n<ul>\n    <li><strong>Guided Backpropagation:</strong> Highlights which parts of the input image influence specific neuron activations.</li>\n    <li><strong>Gradient-weighted Class Activation Mapping (Grad-CAM):</strong> Provides visual explanations for predictions by emphasizing important image regions.</li>\n    <li><strong>Feature Inversion:</strong> Reconstructs the input image from feature representations, showing what information is preserved at each layer.</li>\n</ul>\n\n<h3>11. Future Directions</h3>\n\n<p>If I were to revisit this project, I'd explore more sophisticated models and techniques for visualization. For instance, experimenting with deeper architectures and different types of layers might yield better insights. Additionally, applying these techniques to more complex datasets could provide more nuanced understanding and improved model interpretability.</p>\n\n<h3>12. Conclusion</h3>\n\n<p>Visualizing hidden layers in neural networks is a powerful approach for enhancing model transparency and debugging. By peering into the network's inner workings, we gain valuable insights into its decision-making processes and can potentially identify areas for improvement. As deep learning continues to advance, techniques like these are crucial for developing more interpretable and effective artificial intelligence systems.</p>",
    "time": "24 July 2024",
    "tags": [
      "Visualizing layer",
      "Deep learning",
      "Convolutional Neural Networks"
    ]
  },
  {
    "title": "Need For Speed Self Driving Car",
    "content": "<h3>Training a ConvNet to drive in Need for Speed: Most Wanted</h3>\n\n<p>After making the Mario Reinforcement Learning project my unserstanding about these systems became clearer and new project ideas started coming to mind. One project that I was amazed with was Sentdex (youtuber) GTA 5 self driving series. I first saw that series in 2016 and since then I wanted to train one of my own. But not GTA 5 but NFS Most Wanted.So, we'll explore how to train a convolutional neural network (ConvNet) to play Need for Speed: Most Wanted (2005). This project sits at the intersection of computer vision, reinforcement learning, and good old-fashioned gaming. Let's dive in!</p>\n\n<h3>Problem Formulation</h3>\n<p>Our goal is to create an agent that can navigate a car through the streets of Rockport City. We'll frame this as an image-to-action mapping problem: given a grayscale image of the game screen, our ConvNet needs to output the appropriate steering command.</p>\n\n<h3>Data Collection</h3>\n<p>The first step in any machine learning project is data acquisition. In this case, we need to play the game ourselves and record our actions. Here's a Python script that captures screen images and keyboard inputs:</p>\n\n<pre style=\"background-color: #222; padding: 5px 10px\"><code>import numpy as np\nimport cv2\nfrom grabscreen import grab_screen\nfrom getkeys import key_check\n\ndef keys_to_output(keys):\n    output = [0,0,0]  # [A,W,D]\n    if 'A' in keys: output[0] = 1\n    elif 'D' in keys: output[2] = 1\n    else: output[1] = 1\n    return output\n\n# Main data collection loop\nwhile True:\n    screen = grab_screen(region=(0,40,800,640))\n    screen = cv2.cvtColor(screen, cv2.COLOR_BGR2GRAY)\n    screen = cv2.resize(screen, (160,120))\n    keys = key_check()\n    output = keys_to_output(keys)\n    training_data.append([screen,output])\n</code></pre>\n\n<p>This script runs in the background while we play, continuously adding (state, action) pairs to our dataset.</p>\n\n<h3>Data Preprocessing</h3>\n<p>Once we have our raw data, we need to prepare it for training:</p>\n\n<pre style=\"background-color: #222; padding: 5px 10px\"><code>X = np.array([i[0] for i in data]).reshape(-1, 120, 160, 1)\ny = np.array([i[1] for i in data])\nX = X / 255.0  # Normalize pixel values\n</code></pre>\n\n<h3>Model Architecture</h3>\n<p>For our ConvNet, we'll use a modified InceptionResNetV2 architecture. This might seem like overkill for a racing game, but remember: we're not here to play, we're here to dominate.</p>\n\n<pre style=\"background-color: #222; padding: 5px 10px\"><code>from tensorflow.keras.applications import InceptionResNetV2\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\n\nbase_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(120, 160, 3))\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(1024, activation='relu')(x)\npredictions = Dense(3, activation='softmax')(x)\n\nmodel = Model(inputs=base_model.input, outputs=predictions)\n</code></pre>\n\n<h3>Training</h3>\n<p>With our data prepared and model defined, we can now train our virtual street racer:</p>\n\n<pre style=\"background-color: #222; padding: 5px 10px\"><code>model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n</code></pre>\n\n<h3>Deployment and Testing</h3>\n<p>Now comes the moment of truth. We load our trained model and let it loose on the streets of Rockport:</p>\n\n<pre style=\"background-color: #222; padding: 5px 10px\"><code>model = load_model('nfs_driver_model.h5')\n\nwhile True:\n    screen = grab_screen(region=(0,40,800,640))\n    screen = cv2.cvtColor(screen, cv2.COLOR_BGR2GRAY)\n    screen = cv2.resize(screen, (160,120))\n    prediction = model.predict([screen.reshape(-1,120,160,1)])[0]\n    moves = list(np.around(prediction))\n    \n    if moves == [1,0,0]: left()\n    elif moves == [0,1,0]: straight()\n    elif moves == [0,0,1]: right()\n</code></pre>\n\n<h3>Results and Analysis</h3>\n<p>Our initial results were... interesting. Here are some key observations:</p>\n\n<ol>\n    <li><strong>Data Imbalance</strong>: Our model had a strong bias towards driving straight, likely due to overrepresentation in the training data. This highlights the importance of balanced datasets in imitation learning.</li>\n    <li><strong>Overfitting</strong>: The model showed signs of memorizing specific tracks rather than generalizing driving skills. This suggests we might need more diverse training data or stronger regularization.</li>\n    <li><strong>Action Oscillation</strong>: Rapid switching between left and right turns led to a \"wiggle\" effect. This could potentially be mitigated by incorporating temporal information, perhaps through a recurrent architecture or frame stacking.</li>\n    <li><strong>Computational Constraints</strong>: Running game emulation, screen capture, and neural network inference simultaneously pushed our hardware to its limits. This underscores the need for efficient, real-time inference in game-playing AI.</li>\n    <li><strong>Dynamic Obstacle Handling</strong>: The model struggled with traffic and police cars, indicating a need for more sophisticated environment understanding.</li>\n</ol>\n\n<h3>Future Directions</h3>\n<p>This project opens up several exciting avenues for further research:</p>\n\n<ol>\n    <li><strong>Reinforcement Learning</strong>: Instead of imitation learning, we could frame this as a RL problem, allowing the agent to discover optimal driving strategies through trial and error.</li>\n    <li><strong>Multi-Task Learning</strong>: We could extend our model to handle not just driving, but also higher-level tasks like race completion or police evasion.</li>\n    <li><strong>Curriculum Learning</strong>: Starting with simple, traffic-free scenarios and gradually increasing complexity could lead to more robust agents.</li>\n    <li><strong>Transfer Learning</strong>: Can driving skills learned in Need for Speed transfer to other racing games, or even to real-world autonomous driving simulations?</li>\n</ol>\n\n<h3>Conclusion</h3>\n<p>Training an AI to play Need for Speed: Most Wanted is more than just a fun project—it's a microcosm of the challenges we face in real-world autonomous driving. By tackling these problems in a controlled, virtual environment, we can gain insights that may prove valuable as we work towards fully autonomous vehicles.</p>\n\n<p>Remember, the code for this project is available on GitHub. Feel free to fork it, improve it, and maybe even challenge my AI to a race. May the best neural network win!</p>",
    "time": "18 July 2024",
    "tags": ["Need For Speed", "Deep learning", "Convolutional Neural Networks"]
  },
  {
    "title": "Mario Reinforcement Learning",
    "content": "<p>In this post, we'll explore the application of deep reinforcement learning to the classic NES game, Super Mario Bros. This project serves as an excellent case study in the challenges and intricacies of training an agent to perform complex sequential decision-making tasks.</p>\n\n<h3>Problem Formulation</h3>\n\n<p>Our objective is to train an agent to navigate through levels of Super Mario Bros, maximizing its score while avoiding obstacles. We'll frame this as a reinforcement learning problem, where the agent must learn a policy that maps game states to actions, optimizing for long-term reward.</p>\n\n<h3>Environment Setup</h3>\n\n<p>We'll use OpenAI's Gym library along with a custom Super Mario Bros environment. Here's the basic setup:</p>\n\n<pre style=\"background-color: #222; padding: 5px 10px\"><code>import gym\nimport gym_super_mario_bros\nfrom gym_super_mario_bros.actions import SIMPLE_MOVEMENT\nfrom nes_py.wrappers import JoypadSpace\n\nenv = gym_super_mario_bros.make('SuperMarioBros-v0')\nenv = JoypadSpace(env, SIMPLE_MOVEMENT)\n\nstate_space = env.observation_space.shape[0]\naction_space = env.action_space.n\n\nprint(f\"State space: {state_space}\")\nprint(f\"Action space: {action_space}\")\n</code></pre>\n\n<p>This setup provides us with a discrete action space and a high-dimensional state space (the raw pixel data of the game screen).</p>\n\n<h3>Model Architecture</h3>\n\n<p>For our agent, we'll implement a Deep Q-Network (DQN). The architecture is as follows:</p>\n\n<pre style=\"background-color: #222; padding: 5px 10px\"><code>import torch\nimport torch.nn as nn\n\nclass DQN(nn.Module):\n    def __init__(self, state_space, action_space):\n        super(DQN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n        self.fc1 = nn.Linear(3136, 512)\n        self.fc2 = nn.Linear(512, action_space)\n\n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = torch.relu(self.conv3(x))\n        x = x.view(x.size(0), -1)\n        x = torch.relu(self.fc1(x))\n        return self.fc2(x)\n\ndqn = DQN(state_space, action_space)\noptimizer = torch.optim.Adam(dqn.parameters(), lr=0.00025)\nloss_fn = nn.MSELoss()\n</code></pre>\n\n<p>This architecture uses convolutional layers to process the visual input, followed by fully connected layers to produce Q-values for each action.</p>\n\n<h3>Training Algorithm</h3>\n\n<p>We'll use the DQN algorithm with experience replay and target network. Here's a simplified version of the training loop:</p>\n\n<pre style=\"background-color: #222; padding: 5px 10px\"><code>from collections import deque\nimport random\nimport numpy as np\n\nreplay_memory = deque(maxlen=10000)\ntarget_dqn = DQN(state_space, action_space)\ntarget_dqn.load_state_dict(dqn.state_dict())\n\ndef train_mario(num_episodes=10000, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n    epsilon = epsilon_start\n    for episode in range(num_episodes):\n        state = env.reset()\n        total_reward = 0\n        done = False\n\n        while not done:\n            if random.random() < epsilon:\n                action = env.action_space.sample()\n            else:\n                q_values = dqn(torch.FloatTensor(state).unsqueeze(0))\n                action = torch.argmax(q_values).item()\n\n            next_state, reward, done, _ = env.step(action)\n            replay_memory.append((state, action, reward, next_state, done))\n\n            if len(replay_memory) > 32:\n                batch = random.sample(replay_memory, 32)\n                states, actions, rewards, next_states, dones = zip(*batch)\n\n                states = torch.FloatTensor(states)\n                actions = torch.LongTensor(actions)\n                rewards = torch.FloatTensor(rewards)\n                next_states = torch.FloatTensor(next_states)\n                dones = torch.FloatTensor(dones)\n\n                current_q_values = dqn(states).gather(1, actions.unsqueeze(1))\n                next_q_values = target_dqn(next_states).max(1)[0].detach()\n                target_q_values = rewards + (1 - dones) * 0.99 * next_q_values\n\n                loss = loss_fn(current_q_values, target_q_values.unsqueeze(1))\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            state = next_state\n            total_reward += reward\n\n        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n        if episode % 10 == 0:\n            target_dqn.load_state_dict(dqn.state_dict())\n\n        print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n\ntrain_mario()\n</code></pre>\n\n<h3>Challenges and Observations</h3>\n\n<p>Training an agent to play Super Mario Bros presents several interesting challenges:</p>\n\n<ol>\n    <li><strong>Sparse Rewards</strong>: The game provides rewards infrequently (mainly for collecting coins, defeating enemies, and completing levels). This sparse reward structure can make learning difficult, as the agent receives limited feedback on its actions. We might consider reward shaping to provide more frequent learning signals.</li>\n    <li><strong>High-Dimensional State Space</strong>: Using raw pixel data as input results in a very large state space, which can slow down learning. Techniques like frame stacking or using a more compact state representation (e.g., Mario's position, velocity, and nearby obstacles) could potentially improve learning efficiency.</li>\n    <li><strong>Partial Observability</strong>: The game screen provides only partial information about the game state. The agent cannot see what's coming ahead, which makes long-term planning challenging. This is a common issue in many real-world reinforcement learning problems.</li>\n    <li><strong>Temporal Credit Assignment</strong>: Many actions in Mario have delayed effects (e.g., jumping to avoid an enemy you can't see yet). This makes it difficult for the agent to associate actions with their long-term consequences. Techniques like n-step returns or eligibility traces might help address this issue.</li>\n    <li><strong>Exploration-Exploitation Trade-off</strong>: Balancing exploration of the environment with exploitation of known good strategies is crucial. The epsilon-greedy approach we've used is a simple way to handle this, but more sophisticated methods like intrinsic motivation or parameter noise for exploration could potentially yield better results.</li>\n</ol>\n\n<h3>Future Directions</h3>\n\n<p>This project opens up several interesting avenues for further research:</p>\n\n<ol>\n    <li><strong>Advanced Architectures</strong>: Experimenting with more sophisticated network architectures, such as dueling DQN or distributional DQN, could potentially improve performance.</li>\n    <li><strong>Multi-task Learning</strong>: Training the agent to play multiple Mario levels or even different NES games could lead to more robust and general game-playing agents.</li>\n    <li><strong>Meta-learning</strong>: Investigating meta-learning approaches to allow the agent to adapt quickly to new levels or game variations.</li>\n    <li><strong>Hierarchical Reinforcement Learning</strong>: Implementing a hierarchical approach where high-level policies (e.g., \"complete the level\") are composed of lower-level policies (e.g., \"jump over obstacle\") could potentially handle the complexity of the game more effectively.</li>\n</ol>\n\n<h3>Conclusion</h3>\n\n<p>Training an AI to play Super Mario Bros serves as an excellent case study in deep reinforcement learning. It encompasses many of the challenges we face in more complex, real-world applications of RL, such as robotics or autonomous driving. By tackling these problems in a controlled, simulated environment, we can gain valuable insights that may transfer to more practical applications.</p>\n\n<p>The code for this project is available on GitHub. I encourage readers to experiment with different architectures, hyperparameters, and learning algorithms. There's still much to explore in the world of AI game-playing agents!</p>",
    "time": "10 July 2024",
    "tags": ["mario", "reinforcement learning"]
  },
  {
    "title": "The Essence of Engineering",
    "content": "<p>I have been programming for the past six years—two and a half years in competitive programming and the rest in web development. While competitive programming felt like true engineering, web development, especially frontend, does not. Six years in, I aspire to be an engineer in the truest sense. As people venture to Mars, I'm here centering text in a div. Both are labeled \"Engineering,\" but how can I equate my work to theirs? The difference in impact, skill, and excitement is stark, making me question if I'm truly utilizing my potential. No matter how proficient I become in web development, I'll always feel my mind could be better applied to System Engineering or Machine Learning, tackling the truly challenging problems. Web development may seem filled with unsolvable tasks, but I disagree. At best, it's about novel ideas and doing something new within the browser.</p><p>At the end of the day, coding for the web isn't the bottleneck; it's the idea. Startups thrive on new ideas rather than groundbreaking technology. Innovation in web development is driven by necessity, while fields like machine learning, as of early 2020, still have vast uncharted territories, offering ample room for innovation and new, once-unimaginable ideas. In web development, the outcome is often clear, whereas in fields like AI, the potential is still largely unknown. During my time in web development, I realized I was coding, not programming. I aspire to be a far better programmer, not just a coder. It's time to move beyond the browser and into the real world of engineering.</p><p>I know this might sound like I'm disparaging web development, but I'm not. To each their own. I appreciate web development for what it is. The challenge lies in its complexity, not in its difficulty. It's harder to maintain the big picture as it gets complex, but it follows a set flow and predefined rules, even in backend development. I still enjoy web development at my job. It's not difficult, so I set myself ambitious deadlines to recapture the thrill I felt during competitive programming. Though web development is fun, lucrative, and useful, I don't see it as prestigious. System engineering, machine learning, and solving the unsolvable seem more prestigious to me, perhaps because fewer people venture into those realms.</p><p>With confidence in my technical skills and a couple of unique ideas, I believe I can achieve great things. To me, engineering isn't about replicating well-trodden paths (like web development). True engineering is a way of thinking: compartmentalizing, filtering out the noise, distinguishing between noise and gold, simplifying problems to comprehensible dimensions (mathematics), building on past achievements, viewing problems from multiple perspectives, and leveraging human research across various fields to create seamless solutions. That's what I believe true engineering is.</p>",
    "time": "25 February 2020",
    "tags": ["machine learning"]
  },
  {
    "title": "Pattern Recognition: What's next in tech?",
    "content": "<h3>Humble beginnings: The age of digitization</h3><p>First when computers came to be in 1960s and 70s, they where only used by scientists and the tech savy. It was obvious to the inventors that the real application of this invention is not in scientific research but in the hands of general public. Entrepreneurs like Bill Gates and Steve Jobs among others were the first to realize this, hence putting all their efforts in making computer accessible to the general public for personal use leading to the foundation of Microsoft and Apple. And they where right! Hence, the explosion of applications of computer in every field that existed. Shreading old methods and making everything digital. <b>It was the inventors who started this revolution but, it was the general public who carried this invention to unimaginable heights.</b></p><h3>Pattern in the age of globalization</h3><p>Then in 1980s and 1990s a new invention leveraging computers came to be: The internet. At first it was used only for research and military purposes. But the inventor Tim Berners Lee and his team knew the real power of internet can only be attained when made global, internet was made keeping that in mind but only for Government network at that time. So, on 6th August 1991 internet was opened to the public FOR FREE. All of a sudden an explosion of internet startups occured, hiding away another level of abstraction and making it even easier for general public to use it by creating tools such as internet browser. This revolution was much greater and longer than the previous because it allowed people to connect to each other all around the planet. It served as a precursor, a base on which the age of information was yet to establish itself. <b>Again, the revolution was started by the inventor but it was the general public who carried it through and made the invention what it is right now.</b></p><h3>Pattern in the age of information</h3><p>The mid 1990s but mostly 2000s saw the rise of social networking sites leveraging internet. This started with the invention of Geocities(1994) and AOL instant messenger(1997) where information could be sent and received from anywhere on the planet in miliseconds. The potential of these inventions easily surpassed old methods such as landline, newspaper and the way people connect with each other. This led to another revolution: The fifth estate. Everything could be found on the internet and general public would themself upload their information on the internet. <b>New inventions were made on top of previous inventions(computer, internet) but, it was again the general public driving the revolution.</b></p><h3>Pattern in the age of automation</h3><p>Started mostly in 2010s meaning we are currently living in the age of automation. The age of information generated(and is still generating) unfathomable amount of data every day which lead to the age of automation. Hence, the next big thing: Artificial Intelligence was born (technically, it was born in 1950s but back then we neither had data nor fast enough hardware to process any meaning out of it.) Again, tools were invented by the visionaries who predicted the changing market, social and economic needs. Using the tools countless applications of AI came into existence. <b>The contribution of general public in this age is their data hence, it's again the general public who's responsible for driving the revolution.</b></p><h3>The future: Man and machine as one.</h3><p>The pattern I see everytime is that, <b>at first a technology is invented, then a platform is created by the early ones, the visionaries who realize it's potential, then it's let loose to the general public to leverage that technology and the explosion occurs.</b> One can argue that in the age of automation general public is not directly influencing the revolution. Data is key to automation and that data is produced by general public but still, they are not the ones processing the data, it's the Data Scientists. And I think this argument is correct. It's like Youtube has been invented but, only Youtube employees can upload video. That's the state of AI right now. It needs general public to explode, just like in the case of Youtube, Instagram, Medium etc. But, the age of automation hasn't ended yet.</p><p>And that's the thing I'm trying to figure out before switching from Software Engineer to Machine Learning Engineer. How can one give the power of AI in the hands of general public such that they can implement it the way they want. That would be this age's revolution. AI explosion hasn't happened yet because the power of AI is still in the hands of visionaries and the tech savy. It hasn't yet trickled down to the general public, who matters the most.</p>",
    "time": "14 April 2019",
    "tags": ["pattern", "2019-2025", "Industry 5.0"]
  },
  {
    "title": "Project Updates",
    "content": "<h3>Optimizing Personal Project Architecture: A Technical Review and Roadmap</h3>\n\n<p>As I approach a potential migration to a new AWS instance, it presents an opportune moment to conduct a comprehensive review and refactoring of our existing projects. This process will involve updating dependencies, optimizing performance, and implementing new features. The goal is to leverage the latest advancements in web technologies to create more robust, efficient, and scalable applications.</p>\n\n<h2>Global Optimization Strategies</h2>\n\n<p>Before diving into individual projects, let's outline some overarching strategies that will be applied across all applications:</p>\n\n<ul>\n  <li>Dependency Updates: Upgrade all projects to use the latest stable versions of their respective frameworks and libraries.</li>\n  <li>Progressive Web App (PWA) Implementation: Enhance offline capabilities and performance across all applications.</li>\n  <li>Package Manager Evaluation: Consider migrating from npm to Yarn for potentially faster and more reliable dependency management.</li>\n  <li>Node.js Update: Ensure all projects are running on the latest LTS version of Node.js to leverage performance improvements and new ECMAScript features.</li>\n</ul>\n\n<h2>Project-Specific Optimizations and Feature Implementations</h2>\n\n<h3>1. Empire of the Clouds (ReactJS)</h3>\n\n<p>This project aims to create a universal music streaming and organization application. Key technical requirements include:</p>\n\n<ul>\n  <li>Integration with Google Drive API for cloud storage access</li>\n  <li>Implementation of a high-performance audio streaming protocol</li>\n  <li>Development of a responsive and intuitive UI using React hooks and functional components</li>\n  <li>Integration with Last.fm API for scrobbling functionality</li>\n  <li>Optimization for minimal bundle size and maximum performance</li>\n</ul>\n\n<h3>2. Time my Show (Angular 7 + Apollo + GraphQL)</h3>\n\n<p>This application requires a significant architectural overhaul to improve daily usability. Planned improvements include:</p>\n\n<ul>\n  <li>Migration of backend to GraphQL for more efficient data fetching</li>\n  <li>Implementation of a PostgreSQL or MongoDB database for persistent storage</li>\n  <li>Enhancement of the data model to include seasons and episodes information</li>\n  <li>Optimization of routing logic for improved navigation</li>\n  <li>Implementation of a type-ahead search functionality in the banner</li>\n  <li>Refactoring to leverage Angular 7's performance improvements</li>\n</ul>\n\n<h3>3. Memory Sequence (Angular 7 + MongoDB)</h3>\n\n<p>This heavily-used application will undergo substantial enhancements:</p>\n\n<ul>\n  <li>Implementation of a rich text editor for in-app content creation</li>\n  <li>Development of a categorization system with MongoDB for efficient data organization</li>\n  <li>Integration of writing analytics functionality</li>\n  <li>Implementation of role-based access control for public/private content</li>\n  <li>Optimization of mobile UI/UX with responsive design principles</li>\n  <li>Integration of a spell-checking library, potentially using a machine learning model for context-aware corrections</li>\n  <li>Refactoring of routing logic using Express.js for improved SEO and performance</li>\n</ul>\n\n<h3>4. Town Center (Angular 7)</h3>\n\n<p>This portfolio website will undergo a complete CSS architecture overhaul:</p>\n\n<ul>\n  <li>Migration from custom CSS to a component-based UI library (Bootstrap or Bulma)</li>\n  <li>Implementation of CSS-in-JS or CSS modules for better encapsulation and maintainability</li>\n  <li>Optimization of render performance, potentially leveraging Angular's OnPush change detection strategy</li>\n  <li>Enhancement of visual design with increased use of transparency and modern design principles</li>\n</ul>\n\n<h3>5. Creek (Apollo + GraphQL)</h3>\n\n<p>This project requires a complete architectural redesign to address performance issues:</p>\n\n<ul>\n  <li>Migration from REST to GraphQL for more efficient data fetching</li>\n  <li>Implementation of data caching strategies to reduce API calls</li>\n  <li>Potential migration from Angular to React for a lighter-weight application</li>\n  <li>If keeping Angular, update to version 7 and implement performance optimizations</li>\n  <li>Implementation of server-side rendering for improved initial load times</li>\n</ul>\n\n<h2>Conclusion and Next Steps</h2>\n\n<p>This technical review outlines a comprehensive roadmap for optimizing our project ecosystem. By leveraging modern web technologies and architectural patterns, I aim to significantly improve performance, maintainability, and user experience across all applications.</p>\n\n<p>Next steps involve creating detailed implementation plans for each project, prioritizing based on current usage and potential impact. We'll also need to set up a robust testing infrastructure to ensure that these significant changes don't introduce new bugs or regressions.</p>\n\n<p>As I proceed with these updates, it will be crucial to monitor performance metrics closely and potentially implement A/B testing to quantify the impact of our changes. This data-driven approach will allow us to iterate and further optimize our applications based on real-world usage patterns.</p>",
    "time": "30 December 2018",
    "tags": ["pet projects", "still 2018"]
  },
  {
    "content": "<p>I was trying to embed spotify player in memories. It's a great idea. I should play with spotify and last.fm apis, there could easily be some features I don't know about. Well, I looked through their apis, nothing interesting to use on this website.</p><p>Listen to this song...</p><div class=\"col-md-6 col-sm-12\"><iframe src=\"https://embed.spotify.com/?uri=spotify%3Atrack%3A6ppPNxdwG8A5Ed8DlNLV6I\" width=\"100%\" height=\"80\" frameborder=\"0\" allowtransparency=\"true\"></iframe></div><p>I accidentally listened to this when Spotify played this after <em>Iron Maiden's</em> playlist finished. Fell in love the moment I heard it. Althought in my experience the songs that I like in the first listen are the songs which seem to fade away quickly. </p><p>Anyways, I am using tinyMCE&nbsp;editor. It's nice because it's producing the final HTML in minified format. I just need a way to automate this process. I should look into the features of this editor. Maybe there's a way to convert the HTML into JSON&nbsp;somehow. That would be so easy for me then. Will have to write extra code for user layer for login, that would require me to study MongoDB. Well, let's see how this goes.</p><p>I got an idea, some implementation of adding notes to memories. Although it would require me to manually add them in data.json file which will be tiresome. We will see about that. I have decided to leave data.json to be my primary and quite frankly the only way to update/add memories. I just need to figure out AWS. It's so fucking confusing. I configured it to run blue/green deployment but my server where not stopping, even when I terminated them manually, new servers kept popping up!</p><p>And this is the reason why you should plan your project properly before starting it. I don't have continous integration or any server to put this project on or the final wireframe of the project. When they say that you should wireframe your project first, I hope they know that most probably there will be major changes to the project in the future. I should have wireframed properly eitherways. Now I need to shift everything from base component to a new component of itself such that I can summon it anywhere in the base component as a part of the it. And similarly make new components as a child of the base component. If I had thought about this earlier then it would have saved much of my time now. So, I guess wireframing is important even though you will have to change most of it in the end.</p>",
    "tags": ["computer science", "development", "dio", "wireframe"],
    "time": "28 July 2017",
    "title": "MS#03: Building the website: Wireframing?"
  },
  {
    "content": "<p>I want to write about the project as I don't have any strong feeling towards something or any other substancial thing to write about.I have been working on this project for 12 days now. I am happy with what I've built so far. Angular 4 is an awesome framework! It has made the webapp so modular and versatile that each component can co-exist with every other component and together they are forming different states which basically is one of the permutation states. The app seems to run fast and efficient as I don't have any additional modules and libraries lying around. Dead-code has been minimized although Angular 4's <em>tree-shaking</em> would <em>nail the coffin</em>. The aim was to be light-weight, there was no restrictions on the processing power or anything, I made it such, 'cause <em>why not</em>? Although according to <em>Chrome's</em> console the whole app takes 2.3 seconds to load, which is higher than expected. So, I tried running a new project generated via angular-cli and benchmark it. It was taking 1.6 seconds to load the complete seed project. According to that I guess my app is supposed to take 2.3 seconds. I think <em>Spotify</em> is delaying the app. I see the errors for every <em>Spotify</em> player embedded, in the console. Strangers on the Internet are saying that it's some <em>Google Chrome</em> SSL certificate permission termination error. The solution provided didn't worked.</p><p>The webapp is more or less completed, the only things left is the admin layer and search linking. I have found the solution as to how I can make this webapp public and at the same time use it the way I wanted. The idea is to add an admin layer such that all the memories with \"personal\" tag on them will only be shown to me. These personal memories will simply be excluded from the final public JSON object. I will code it tomorrow I guess. The next problem would be to rent a Virtual Private Server. I am afraid in using my Dad's card for paying on services which automatically charge you after your free subscription gets over. I am very lazy and careless at this. I need a company where it's paid monthly or completly free, although the later is hard to find. It just happened to me the other day, I rented a VPS from <em>Amazon</em> and it was fine and working, I even checked <em>blue/green deployment</em>, it all worked. After getting bored off playing with it, I went on to some other project. Four days later, I found, while I was messing with AWS I started four instances and all of them have been continuously running since. The free(which is $1) limit was 750hrs and I was doing 940hrs.</p><p>Tomorrow I definitely will score a server. I hope Digital Ocean has <em>Continuous Integration</em>. If I am able to deploy it and pipe it to Github, I can automate the process of updating the content on the website.</p>",
    "tags": ["computer science", "development", "web app update"],
    "time": "2 August 2017",
    "title": "MS#04: Web App Update"
  },
  {
    "content": "<p>Memory Sequences is almost complete, one more day might do it. I am currently well-versed in Angular4 + Typescript and I have 3 more web apps to make using this framework which is good but I want to make apps using GoLang&nbsp;and Ionic Framework as well. I am thinking of giving ReactJs a run too. People are saying it's light-weight and easy to learn. It's going head-to-head against Angular4, it has to be good. But then I also want to explore&nbsp;C++'s development side by making a project. And if I'm doing C++ then I might as well make games. That would check another thing off the bucket-list. Although&nbsp;I want to get in machine learning too, I've never paid much attention to it in the past but it seems that's where the future's headed. That makes 5 completely(more or less) new domains to be learned by me in the next 9 months. It would feel overwhelming if I make a schedule to do all these projects. Schedules never work with me any ways. I'm not going to stress over completing all of these but right now I want to do something new and that's why I need to decide on something soon.</p><p>\"<em><strong>Hofstadter's Law:</strong> It always takes longer than you expect, <br />even when you take into account Hofstadter's Law</em>.\"&nbsp;&mdash;&nbsp;Douglas Hofstadter</p><p>It is probable that I won't complete all of these in time so, I have decided to divide them on the basis of urgency of the projects a&nbsp;short-term goal and a long-term goal. Skills in short-term goal are those which I need for mediocre jobs most probably in a start-up, keeping myself on the forefront of technology will help here. For completing short-term goals I think I should continue doing web dev and try learning the obscure parts such as RxJs, ReactJs, make software I need to automate my life thus, encouraging my sloth behavior and eventually taking me to the next level of laziness. Similarly for long-term goal i.e aiming for a job in really good startup most probably abroad. Getting sponsored for a visa is getting harder, although it's relatively easier for developers. Some European countries have a special job skills which a migrant can have, fortunately, software engineering related skills are the ones most in demand. For these startups and companies, I will require having a wider knowledge in computer science. I come from an algorithm(C++) background so I can say a huge &amp; a fairly difficult part is already covered. Game development in C++ would teach me the other side of the language the Object Oriented Programming side. I need to learn machine learning as well. It seems really fun. Actually, I don't know shit about machine learning, I ain't got anything else to write about it other than that the projects are in a different dimension and I really want to try it. I think all this would enstate me as a programmer with fairly balanced knowledge in computer science. According to me and quote me on this, \"becoming a jack of all trades and master of none seems fruitful in software development.\"</p><p>I was of bored of programming so I decided to think and write about the technologies I should learn next. Making plans is so stupid but I still did it. Just like millions times before this plan will also fail, I will complete them if they stay fun otherwise I will give up, I'm not going to stress over them. Actually you can't call this a full-fledged plan. There's a deadline but I won't care much if I crossed it and I haven't even decided the order of the project. I just wanted to layout my interests and decide which one to do first.</p>",
    "tags": ["computer science", "development"],
    "time": "7 August 2017",
    "title": "MS#05: Do or do not, there is no try"
  },
  {
    "content": "<p>I've started building the next project, \"Empire of the clouds\", I named it after one of the most iconic song from Iron Maiden, an absolute eighteen minute marvel. The song is about the legendary airship R101, I went with this name because this webapp is music based and the whole point of this webapp is to store everything on the cloud, considering my music collection an <em>empire</em>. Anyways, I'm writing in typescript and using angular 4, for the last time I swear. Actually, I guess I will make one desktop app for windows and Linux using angular 4, nodejs and electron.</p><p>I am so psyched to start making this one. I'm finally doing something for improving my music library's accessibility and portability. I'm curently listening to non-metal songs on a webapp made by some young programmer. I feel such freedom using this app. All I need is internet and my google drive password and I'll have access to my music. My carefully hand-picked music and playlists all in 320kbps. Time and again I am forgetting where the music is playing from. When I'm realizing it's from the webapp, and that there's no relation to local storage or this computer, it's making me feel so secure and free.</p><p>I've wireframed the app and written all the features I need in two parts: essential and luxury features. It would be hard to build this one. I don't know most of the concepts this app would require. </p><img style=\"padding: 10px 0px\" src = 'https://hmu6bg.bn1301.livefilestore.com/y4mZOoLRylqNFJoe-4ilheJJltn-DXvicuOeM0irpTnZNyzDAEBO-i859v67wfxE3dPIW3rnmg2oy33RlrOGQMzbyqYDRv_Rejla0whrqoNI9Inw6Np4UDZYhUlW2-E_f70VqGm0ppo7kZ8MT8k6pE5_nknSUCyYe1yCyQJdxskc6lkf1Z_EozWwZDdfqIfiOg67rQ81Qim8F1Kz5WhzovvfQ?width=1024&height=765&cropmode=none' width='100%'><p>Well, I made Memory Sequence in angular 4 and typescript when not knowing shit about them. Making this one requires really good knowledge of the framework. I don't know if I will be able to complete it but I will try. </p><p>The webapp I'm using is doing it's job fine but I need way more features. I want to custom make the software for myself. Make it work exactly the way I want and when I want. Also this app doesn't feel my type, there's no surprise and the design is crappy. I won't feel complete freedom unless I use the things I wrote and can control. I am thinking of parsing artist data from last.fm rather than linking to last.fm's page. This would make it lengthy but better.</p><p>In a nutshell I just need to implement Google auth and then further figure out the api to implement drive methods, once able to receive from drive I need to make a player and search for how to play audio using typescript then I will have to make the directory in tree structure in the sidebar so that I can drag and drop the artist I want to play and btw I need to implement drag and drop too, once the player is running using the songs from google drive I need to implement last.fm apis and make pages for artists i.e content, inside content component 3 types of pages would open song list, artist info and last.fm stats for the artist and last but surely not the least, completely redesign the app for mobile because it needs to look completely different and auto-responsiveness won't work.</p><p>And if I want luxury features too, then add yet another paragraph like that.</p>",
    "tags": ["web dev", "empire of the clouds", "computer science"],
    "time": "27 August 2017",
    "title": "Project#15: Empire of the clouds"
  },
  {
    "title": "Project#16: Code Scrapper",
    "content": "<p>I have started with a new project. I need a code downloader for downloading my codes from codeforces and uploading them to github automatically. I'm making this for Linux, Windows and Mac, therefore, I see some future use of this desktop app not only by me but for anyone who want to save their code locally on whichever platform they want. I am using Javascript, nodejs and electron framework for this. </p><p><u>Will build the app and post here simultaneously.</u> <a target=\"_blank\" href='https://github.com/strcoder4007/codeScrapper'><u>Github Repository</u></a></p><h4><u>Day#1</u></h4><p>Wireframing is done. All I need to do now is to learn how to make desktop apps using electron and make it.</p><p>I've been learning this framework for the past hour and have made a useless but working app, successfully running on my 64 bit Arch Linux.</p><p>A little more decisiveness goes a long way. I did the exact opposite. Added bootstrap and then decided to do something new, therefore, instead bootstrap I'm going to use Materialize CSS. It's new and built on top of Angular Material. I thought of using Angular Material 2 directly but it's too early to use that, even the documentation is not yet ready. I've setteled on Materialize CSS. I haven't yet read it's documentation.</p><p>I have to say, Electron is awesome, but it has yet to go far. Many things are just manual, which I think they can automate in future versions making development faster. Electron is pretty straight forward. It uses IPCrenderer (equivalent to @output/@input in Angular 4) for communication between windows (equivalent to components in Angular 4). Basically the communication is the same as in socket.io.</p><p>Went with frameless window for linux and windows. For mac it's native traffic color is used. I am liking Matirialize CSS, it's new and therefore it has an easy grid system and implementing something like blockquotes and truely responsive texts is already built-in. Also, it uses my favourite font&mdash;Roboto as default font. No more <code style=\"background-color: #111\">npm install --save roboto-fontface</code> haha!</p><h4><u>Day#2</u></h4><p>I'm confused with the way electron handles data flow, if it even does or not. Should I just use ReactJs? or Angular4? No, not angular, it will unnecessarily make this project bloated. There is a way in electron I guess, by using sections. The thing is, electron doesn't divide the application into components, like Angular or React. Therefore, without those frameworks implementing this seems hard. I need to add and remove several nodes from DOM at certain states, there seems no feature like this in electron. I will have to use Vanilla Javascript or one of those two.</p><p>Things have started to make sense now.</p><p>Linux version is working fine, windows version is facing issues and no idea about macos version because I'm broke, I got no money to buy a mac. Will have to find some online tool to check macos version.</p><p>Frontend is almost done. How electron handles Inter-process communications is now clear.</p><p>Frustrated, lost 2 hours of code. I packaged the application for windows (.exe setup). It was created in the root folder itself. I was committing the progress on github, forgot that setup files are also present in the root and skipping the setup files for uploading was not declared in .gitignore file. Therefore, it started to upload 70mb of those setup files. Then I cancelled it and declared it in .gitignore and committed again. Didn't work because git had already staged it. I messed up everything and long story short I lost my code. Now I will have to code it again. My mistake.</p><h4><u>Day#3</u></h4><p>Not going to work on this project today, maybe a little bit if I feel like. Today I need to practice algorithms. I need it for an interview couple of days from now.</p><h4><u>Day#4</u></h4><p>Frontend is complete. Had to re-write the lost code. API's are working therefore now I just need to learn how to authenticate using SPOJ's APIs and download the code. Downloading from Codeforces doesn't require me to authenticate.</p><h4><u>Day#5</u></h4><p>The interface is done. The API is working fine. Although, it will take a couple of seconds to download solutions of user with large problem count. I'm making GET requests to the API around 200,000 times whenever the download button is clicked. If anyone has more solutions than that then, my bad. The main code for requests and parsing the \"Accepted\" solutions was fairly small. Written in vanilla JS I was able to place this block of code in the main file itself which electron uses. </p><p>Anyways, the project is almost done. Took around 9 hours to complete without coding the luxury features. This application can be made a lot better but still, a working version is complete and I want to move on to another project.</p>",
    "time": "28 September 2017",
    "tags": ["desktop app", "javascript", "electron", "nodejs", "cs", "web dev"]
  },
  {
    "title": "Leaving competitive programming",
    "content": "<p>As the number of fields in computer science endlessly seem to grow, it's my time to move on. Competitive programming was the first thing I picked up in college. It was recommended to me by my best friend, in 2nd semester. I was learning C from a textbook at that time. I still remember the day I was not able to understand what are test cases and how the program has to be run/submitted on SPOJ. Although, I seriously started with competitive programming 6 months  later. As I didn't had computer science in high school, I had to learn some languages first. I learned C, python and haskell (you can guess how much I know this now) my fresher year. Then came back to competitive programming.</p><p>I don't want to write about my journey here. In short, I practiced around 150 ad-hoc questions on hackerrank and mathematics questions on project euler. Then slowly I went further deep into competitive programming. Starting with codeforces certainly did mark that. Practiced the hardest I had ever on codeforces, more than 500 ad-hoc questions in the span of 4 months including 14 days holidays. How's that for implementation skill? Suddenly I am among the people who know stuff. Focused more on algotithms, learned all graph alorithms, practiced the SPOJ. Then learned more obsecure ones such as fenwick tree, matrix exponentiation and segment trees. Now I am among the good-ish programmers.</p><p>Then came the motivation shift. It was easy to quickly secure the first place in college leaderboard because no one before me took competitive programming seriously. Having the first place on the leaderboard everytime made everyone come after me. Obviously, having 1st place doesn't matter shit. It was the fun, I was after. I explained it to myself, comparing myself to my competitors is worthless. But I couldn't save myself from getting into that loop of hell. My motivation shifted, I no longer pursued fun, I pursued 1st rank. And everything came falling down. I just can't do anything I don't like. My practice became stagnant. Web development was different and new for me. Fast forward a year of constantly starting and leaving competitive programming and without making any progress. I gave up competitive programming. Moved on to web dev.</p><p>After not thinking about competitive programming for 4 months. I've started to feel the same way I used to in 2015. It's fun again. I've given up on trying. Now, I actually don't give a fuck. I can again pursue just fun in competitive programming. So, I've decided to move on to machine learning as I've made a resonable number of projects in web development. And as for competitive programming, I will do it whenever I feel like. I don't have a goal. Therefore, I don't have a strong motivation for improving. If I feel like, I will learn new algorithms as well. Total freedom.</p>",
    "time": "25 October 2017",
    "tags": ["competitive programming"]
  },
  {
    "title": "Project#17: Langref",
    "content": "<p>Don't feel like doing competitive programming or machine learning therefore, building a new app. This app would help learn new languages fast. I am going to use reactjs for this. I am fairly new to reactjs and I don't have any recent projects in reactjs. This is probably the last web dev project I'm going to do.</p><p>This app would compare certain syntaxes of multiple languages side by side so that you can see the syntactic difference. Syntaxes such as if, for, while, foreach, return, function(), arrays, lists, vectors, sets, slicing, string functions etc. I'm a little rusty in reactjs. And as you know in javascript's scene, react would have changed so much since the time I last used it, I rather learn it again. Haven't yet decided on CSS library. Just checked, bootstrap 4 is out now. I am gonna use that. Will use nodejs, npm is good, don't to like copy-paste every dependency. New git repository created.</p><p><u>Will build the app and log here simultaneously</u></p><p>Learning reactjs.</p>",
    "time": "25 October 2017",
    "tags": ["project", "language reference", "reactjs"]
  },
  {
    "title": "Project#18: Ionic Messenger",
    "content": "<p>As the name suggests, this is a hybrid app for both android and iOS using Ionic 3 framework. Ionic framework heavily depends on Angular framework and that's awesome for me. After installing cordova and ionic I went on to open package.json to see the versions of all the dependencies used. To my surprise Ionic 3 uses Angular 5! It was just released a month ago and they have a full support for Angular 5 already with all of it's modules updated to version 5. That was cool to see. Now I will have to learn the concepts introduced in Angular 5 as well.</p><p>The interface of this app will be simple for now because I will concentrate on implementing the backend features I have in my mind. The aim of this project is to demonstrate the obsecure but still useful features of a real time messaging app. The features I have in mind are as follows:<br><ul><li>Have a bot integration.</li><li>Send messages at a specific time set by you.</li><li>Bot to mimic as user. Auto pilot mode. Like sending \"yesterday's photos to a friend\" automatically when that friend asks.</li><li>Going private on click. Either of the user can ask the other to go private. In private mode messages will have constricted length and will get permanentally deleted after 5 seconds.</li></ul></p><p>These are the features I have in mind right now. I'm not sure if I will be able to implement the bot to mimic because there is not much data available for any particular user for the neural net to learn. I need large amount of structured data to train the neural net. Well, some features will be excluded and some will be included as I continue making the app. Will list all of them above.<br><u>Will build the app and log here simultaneously</u></p><h4><u>Day#1</u></h4><p>I've wireframed the app. It's very similar to your standard messaging app. Nothin fancy added there. The app created and <a target=\"_blank\" href='https://github.com/strcoder4007/ionic_messenger'><u>github repo</u></a> created.</p><p>Currently in the process of learning Ionic 3 Framework. Components in Angular is named simply Pages in Ionic. The design of the router is done keeping mobile in mind. Here the components/pages you add are stacked on top of other. Push and pop is done in order to navigate throug pages. Push and pop are available functions and the router is the same as stack of pages.</p><p>Started making the app. Started with coding a little frontend so that the backend can be tested later. The home page is done on the frontend part. No backend yet.</p><h4><u>Day#2</u></h4><p>populated the users with json placeholders. Placed the tab on top.</p><p>Made the chat page. Used AlertController and ActionSheetController for asking to go private and other options.</p><p>Tested for both android and iOS, both working fine.</p><p>I really like how data moves in Ionic. There are many ways for components/page to talk including the ones using Angular's service and eventemitters. Ionic has it's own way as well using navPrams. It's from the NavigationController, amazing!!! Angular should implement the same in their router module, that would make talking much easier.</p><p>Cleaned the app. Removed the dead code.</p><p>I just found out that I can test this app on my mobile in real-time and that too without any extra work. A duplicate server runs on the wifi network. That is such a brilliant idea. Mind = Blown.</p>",
    "time": "16 November 2017",
    "tags": ["project", "web dev", "angular", "ionic"]
  },
  {
    "title": "Project#19: Creek",
    "content": "<p>This is going to be a simple web app usable right out of the box. Writing this in typescript using angular5 and nodejs (yet again). This is one of those apps that I'm gonna use on daily basis, aiding me in keeping myself up to date with the new albums from the artists I love. Now, adding my favourite artists is not going to be done manually! Using last.fm api I will get all the artists I've listented to this far. This list will be full of artists which I don't want as well. Therefore, I will filter the artists whose atleast N songs I've listened. Where N can be set by the user depending on his/her needs. I'm setting it to 10 by default.</p><p>So, basically I want something where I could go and instantly get oncoming albums. I don't need to make any extra effort at all for this app to work. No artists to add, literally.</p><p><u>Will build the app and log here simultaneously</u></p><h4><u>Day#1</u></h4><p>Created the structure of the app. Added dependencies and committed to <a target=\"_blank\" href='https://github.com/strcoder4007/creek'><u>github</u></a></p><p>Started frontend. Will use the method I like better. Gonna make skeleton frontend for the backend to work. When the backend gets completed I will code the frontend. Also, right now I'm not able to come up with a good, fresh and clean frontend.</p>",
    "time": "25 November 2017",
    "tags": [
      "project",
      "web dev",
      "angular",
      "nodejs",
      "yet another nodejs app"
    ]
  }
]
